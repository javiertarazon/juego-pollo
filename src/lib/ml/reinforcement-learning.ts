// Sistema de Aprendizaje por Refuerzo para Predicci√≥n de Posiciones
import { db } from '@/lib/db';
import {
  analizarUltimasPartidas,
  calcularScoreSeguridad,
  detectarRotacionActiva,
} from './adaptive-pattern-analyzer';

// Zonas fr√≠as opuestas (dividir tablero en 2 zonas)
const COLD_ZONES = {
  ZONE_A: [1, 2, 3, 4, 5, 11, 12, 13, 14, 15], // Mitad superior
  ZONE_B: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25], // Mitad inferior
};

// Posiciones siempre seguras por zona - M√çNIMAS para m√°xima diversidad
const SAFE_POSITIONS_BY_ZONE = {
  ZONE_A: [7], // Solo 1 posici√≥n "segura" en zona A
  ZONE_B: [23], // Solo 1 posici√≥n "segura" en zona B
};

interface MLState {
  epsilon: number; // Factor de exploraci√≥n (0-1)
  totalGames: number; // Total de partidas jugadas
  consecutiveSafePositions: number[]; // √öltimas 7 posiciones seguras usadas
  lastZoneUsed: 'ZONE_A' | 'ZONE_B'; // √öltima zona utilizada
  positionQValues: Record<number, number>; // Q-values por posici√≥n (aprendizaje)
  positionSuccessRate: Record<number, { wins: number; total: number }>; // Tasa de √©xito
  explorationCount: number; // Contador de exploraciones
  // NUEVO: An√°lisis adaptativo
  lastAdaptiveAnalysis: Date | null;
  adaptiveScores: Record<number, number>; // Scores adaptativos por posici√≥n
}

// Estado global del ML (en producci√≥n, esto deber√≠a estar en DB)
let mlState: MLState = {
  epsilon: 0.3, // 30% exploraci√≥n inicial
  totalGames: 0,
  consecutiveSafePositions: [],
  lastZoneUsed: 'ZONE_A',
  positionQValues: {},
  positionSuccessRate: {},
  explorationCount: 0,
  // NUEVO
  lastAdaptiveAnalysis: null,
  adaptiveScores: {},
};

// Par√°metros de aprendizaje - FASE 2: ULTRA AGRESIVO + ADAPTATIVO
const LEARNING_RATE = 0.15; // Alpha: aumentado para aprender m√°s r√°pido de errores
const DISCOUNT_FACTOR = 0.85; // Gamma: reducido para priorizar recompensas inmediatas
const MIN_EPSILON = 0.35; // Epsilon m√≠nimo aumentado a 35% para M√ÅXIMA exploraci√≥n
const EPSILON_DECAY = 0.998; // Degradaci√≥n m√°s lenta para mantener exploraci√≥n
const SAFE_SEQUENCE_LENGTH = 15; // Memoria aumentada a 15 para evitar repeticiones
const ADAPTIVE_ANALYSIS_INTERVAL = 60000; // Actualizar an√°lisis cada 60 segundos
const ADAPTIVE_WEIGHT = 0.4; // Peso del an√°lisis adaptativo (40%)

/**
 * Actualizar an√°lisis adaptativo si es necesario
 */
async function actualizarAnalisisAdaptativo(): Promise<void> {
  const ahora = new Date();
  const ultimoAnalisis = mlState.lastAdaptiveAnalysis;

  // Actualizar si nunca se ha hecho o si pas√≥ el intervalo
  if (!ultimoAnalisis || (ahora.getTime() - ultimoAnalisis.getTime()) > ADAPTIVE_ANALYSIS_INTERVAL) {
    console.log('üîÑ Actualizando an√°lisis adaptativo de √∫ltimas 10 partidas...');
    
    try {
      // Analizar √∫ltimas 10 partidas
      const analisis = await analizarUltimasPartidas(10);
      
      // Actualizar scores adaptativos para cada posici√≥n
      for (let pos = 1; pos <= 25; pos++) {
        const scoreData = await calcularScoreSeguridad(pos, 10);
        mlState.adaptiveScores[pos] = scoreData.score / 100; // Normalizar a 0-1
      }

      // Detectar rotaci√≥n activa
      const rotacion = await detectarRotacionActiva(10);
      if (rotacion.hayRotacion) {
        console.log(`üîÑ Rotaci√≥n detectada: ${rotacion.descripcion} (${rotacion.confianza.toFixed(1)}% confianza)`);
      }

      // Mostrar zonas calientes
      if (analisis.zonasCalientes.length > 0) {
        console.log(`üî• Zonas calientes: ${analisis.zonasCalientes.slice(0, 5).map(z => `${z.posicion}(${z.frecuencia.toFixed(0)}%)`).join(', ')}`);
      }

      mlState.lastAdaptiveAnalysis = ahora;
    } catch (error) {
      console.error('‚ùå Error en an√°lisis adaptativo:', error);
    }
  }
}

/**
 * Inicializar Q-values para todas las posiciones
 */
export function initializeMLState() {
  for (let pos = 1; pos <= 25; pos++) {
    if (!mlState.positionQValues[pos]) {
      mlState.positionQValues[pos] = 0.5; // Valor neutral inicial
    }
    if (!mlState.positionSuccessRate[pos]) {
      mlState.positionSuccessRate[pos] = { wins: 0, total: 0 };
    }
    if (!mlState.adaptiveScores[pos]) {
      mlState.adaptiveScores[pos] = 0.75; // Score neutral inicial
    }
  }
}

/**
 * Obtener posiciones "calientes" (usadas 2+ veces en √∫ltimas 5 partidas)
 * MEJORADO: Integra an√°lisis adaptativo
 */
async function getHotPositions(): Promise<number[]> {
  try {
    // Actualizar an√°lisis adaptativo si es necesario
    await actualizarAnalisisAdaptativo();

    // Obtener zonas calientes del an√°lisis adaptativo
    const analisis = await analizarUltimasPartidas(10);
    const zonasCalientes = analisis.zonasCalientes
      .filter(z => z.frecuencia >= 30) // M√≠nimo 30% de frecuencia
      .map(z => z.posicion);

    if (zonasCalientes.length > 0) {
      console.log(`üî• Posiciones CALIENTES detectadas (evitar): ${zonasCalientes.join(', ')}`);
    }

    return zonasCalientes;
  } catch (error) {
    console.error('Error obteniendo posiciones calientes:', error);
    return [];
  }
}

/**
 * Cargar estado del ML desde la base de datos
 */
export async function loadMLState() {
  try {
    // SOLO PARTIDAS REALES para entrenamiento ML
    const games = await db.chickenGame.findMany({
      where: { isSimulated: false }, // Solo partidas reales
      orderBy: { createdAt: 'desc' },
      take: 200, // Aumentado para mejor an√°lisis
      include: { positions: true },
    });

    mlState.totalGames = games.length;

    // Inicializar contadores para TODAS las posiciones
    for (let pos = 1; pos <= 25; pos++) {
      if (!mlState.positionSuccessRate[pos]) {
        mlState.positionSuccessRate[pos] = { wins: 0, total: 0 };
      }
    }

    // Calcular tasa de √©xito general de las √∫ltimas 30 partidas
    const last30Games = games.slice(0, 30);
    const victorias = last30Games.filter(g => !g.hitBone).length;
    const tasaExitoGeneral = (victorias / last30Games.length) * 100;
    
    console.log(`üìä Tasa de √©xito √∫ltimas 30 partidas: ${tasaExitoGeneral.toFixed(1)}%`);
    
    // RESET ADAPTATIVO: Si tasa de √©xito < 48%, resetear Q-values (m√°s sensible)
    if (tasaExitoGeneral < 48 && mlState.totalGames > 30) {
      console.log('üîÑ RESET ADAPTATIVO: Tasa de √©xito muy baja, reseteando Q-values');
      for (let pos = 1; pos <= 25; pos++) {
        mlState.positionQValues[pos] = 0.5; // Resetear a neutral
        mlState.positionSuccessRate[pos] = { wins: 0, total: 0 };
      }
      mlState.epsilon = 0.40; // Aumentar exploraci√≥n despu√©s de reset a 40%
      mlState.consecutiveSafePositions = [];
      console.log('‚úÖ Q-values reseteados, epsilon aumentado a 40%');
    }

    // Calcular tasas de √©xito por posici√≥n SOLO con partidas reales
    games.forEach((game) => {
      const revealed = game.positions
        .filter((p) => p.revealed && p.revealOrder !== null)
        .sort((a, b) => (a.revealOrder || 0) - (b.revealOrder || 0));

      // Analizar TODAS las posiciones reveladas, no solo la primera
      revealed.forEach((pos, index) => {
        const position = pos.position;
        const wasSuccess = pos.isChicken;

        if (!mlState.positionSuccessRate[position]) {
          mlState.positionSuccessRate[position] = { wins: 0, total: 0 };
        }

        mlState.positionSuccessRate[position].total++;
        if (wasSuccess) {
          mlState.positionSuccessRate[position].wins++;
        }

        // Calcular Q-value balanceado
        const successRate =
          mlState.positionSuccessRate[position].wins /
          mlState.positionSuccessRate[position].total;
        
        // Peso balanceado: 60% tasa de √©xito + 40% frecuencia de uso (priorizar diversidad)
        const usageWeight = Math.min(mlState.positionSuccessRate[position].total / 50, 1);
        const balancedQValue = (successRate * 0.6) + (usageWeight * 0.4);
        
        // Penalizar posiciones con 100% de √©xito pero pocos datos
        if (successRate === 1.0 && mlState.positionSuccessRate[position].total < 5) {
          mlState.positionQValues[position] = 0.6; // Reducir confianza m√°s
        } else if (successRate < 0.5 && mlState.positionSuccessRate[position].total > 2) {
          // Penalizar BRUTALMENTE posiciones con < 50% √©xito
          mlState.positionQValues[position] = Math.max(0.1, balancedQValue * 0.3);
        } else if (successRate < 0.4 && mlState.positionSuccessRate[position].total > 3) {
          // Penalizar a√∫n m√°s fuerte si < 40% √©xito
          mlState.positionQValues[position] = Math.max(0.05, balancedQValue * 0.2);
        } else {
          mlState.positionQValues[position] = balancedQValue;
        }
      });
    });

    // Penalizar posiciones sin datos
    for (let pos = 1; pos <= 25; pos++) {
      if (mlState.positionSuccessRate[pos].total === 0) {
        mlState.positionQValues[pos] = 0.5; // Valor neutral
      }
    }

    // Obtener √∫ltimas 15 posiciones seguras usadas (aumentado de 10)
    const recentSafeGames = games
      .filter((g) => !g.hitBone)
      .slice(0, SAFE_SEQUENCE_LENGTH);

    mlState.consecutiveSafePositions = recentSafeGames
      .map((g) => {
        const revealed = g.positions
          .filter((p) => p.revealed && p.revealOrder !== null)
          .sort((a, b) => (a.revealOrder || 0) - (b.revealOrder || 0));
        return revealed.length > 0 ? revealed[0].position : null;
      })
      .filter((p) => p !== null) as number[];

    // Degradar epsilon basado en total de partidas REALES
    mlState.epsilon = Math.max(
      MIN_EPSILON,
      0.35 * Math.pow(EPSILON_DECAY, mlState.totalGames)
    );

    console.log(`ML State cargado: ${mlState.totalGames} partidas REALES, epsilon: ${mlState.epsilon.toFixed(3)}`);
    console.log(`Posiciones con datos: ${Object.values(mlState.positionSuccessRate).filter(s => s.total > 0).length}/25`);
  } catch (error) {
    console.error('Error cargando ML state:', error);
    initializeMLState();
  }
}

/**
 * Determinar si debe explorar o explotar
 */
function shouldExplore(): boolean {
  return Math.random() < mlState.epsilon;
}

/**
 * Obtener zona opuesta a la √∫ltima usada
 */
function getOppositeZone(): 'ZONE_A' | 'ZONE_B' {
  return mlState.lastZoneUsed === 'ZONE_A' ? 'ZONE_B' : 'ZONE_A';
}

/**
 * Verificar si una posici√≥n puede ser usada (no en √∫ltimas 7 seguras)
 */
function canUsePosition(position: number): boolean {
  // Si la memoria tiene menos de 7 posiciones, permitir todas
  if (mlState.consecutiveSafePositions.length < SAFE_SEQUENCE_LENGTH) {
    return true;
  }
  
  // Si la memoria est√° llena, verificar que no est√© en ella
  return !mlState.consecutiveSafePositions.includes(position);
}

/**
 * Obtener posiciones disponibles en una zona
 */
function getAvailablePositionsInZone(
  zone: 'ZONE_A' | 'ZONE_B',
  revealedPositions: number[]
): number[] {
  const zonePositions = COLD_ZONES[zone];
  const safePositions = SAFE_POSITIONS_BY_ZONE[zone];

  return safePositions.filter(
    (pos) =>
      !revealedPositions.includes(pos) && // No revelada en partida actual
      canUsePosition(pos) // No en √∫ltimas 7 seguras
  );
}

/**
 * Seleccionar posici√≥n usando estrategia epsilon-greedy
 */
export async function selectPositionML(
  revealedPositions: number[] = []
): Promise<{
  position: number;
  strategy: 'EXPLOIT' | 'EXPLORE';
  zone: 'ZONE_A' | 'ZONE_B';
  epsilon: number;
  qValue: number;
  confidence: number;
}> {
  // Cargar estado si es primera vez
  if (mlState.totalGames === 0) {
    await loadMLState();
    initializeMLState();
  }

  // Obtener posiciones calientes (a evitar)
  const hotPositions = await getHotPositions();

  // Obtener TODAS las posiciones disponibles (excluyendo calientes)
  const allAvailable = Array.from({ length: 25 }, (_, i) => i + 1).filter(
    (p) => 
      !revealedPositions.includes(p) && 
      canUsePosition(p) &&
      !hotPositions.includes(p) // NUEVO: Evitar posiciones calientes
  );

  // Si no hay posiciones disponibles (memoria llena o todas calientes), relajar restricciones
  let finalAvailable = allAvailable;
  if (finalAvailable.length === 0) {
    console.log('‚ö†Ô∏è  No hay posiciones disponibles, relajando restricciones...');
    // Primero intentar sin memoria pero manteniendo filtro de calientes
    finalAvailable = Array.from({ length: 25 }, (_, i) => i + 1).filter(
      (p) => !revealedPositions.includes(p) && !hotPositions.includes(p)
    );
    
    // Si a√∫n no hay, ignorar tambi√©n calientes (√∫ltimo recurso)
    if (finalAvailable.length === 0) {
      console.log('‚ö†Ô∏è  Ignorando posiciones calientes por necesidad');
      finalAvailable = Array.from({ length: 25 }, (_, i) => i + 1).filter(
        (p) => !revealedPositions.includes(p)
      );
    }
  }

  // Determinar zona para anti-detecci√≥n
  const targetZone = getOppositeZone();
  const zonePositions = COLD_ZONES[targetZone];
  
  let selectedPosition: number;
  let strategy: 'EXPLOIT' | 'EXPLORE';

  // Decisi√≥n: Explorar o Explotar
  if (shouldExplore()) {
    // EXPLORACI√ìN: Selecci√≥n aleatoria de TODAS las posiciones disponibles
    strategy = 'EXPLORE';
    selectedPosition =
      finalAvailable[Math.floor(Math.random() * finalAvailable.length)];
    mlState.explorationCount++;
  } else {
    // EXPLOTACI√ìN: Seleccionar mejor Q-value
    strategy = 'EXPLOIT';
    
    // Actualizar an√°lisis adaptativo si es necesario
    await actualizarAnalisisAdaptativo();
    
    // Calcular score combinado: Q-value + bonus de zona + penalizaciones AGRESIVAS + SCORE ADAPTATIVO
    const positionsWithScores = finalAvailable.map((pos) => {
      const qValue = mlState.positionQValues[pos] || 0.5;
      const usageCount = mlState.positionSuccessRate[pos]?.total || 0;
      const successRate = mlState.positionSuccessRate[pos]?.wins || 0;
      const failureRate = usageCount - successRate;
      
      // NUEVO: Score adaptativo basado en √∫ltimas 10 partidas
      const adaptiveScore = mlState.adaptiveScores[pos] || 0.75;
      
      // Bonus por estar en zona objetivo (muy reducido)
      const zoneBonus = zonePositions.includes(pos) ? 0.02 : 0;
      
      // Penalizaci√≥n BRUTAL por uso excesivo - FASE 2
      let diversityPenalty = 0;
      if (usageCount > 4) {
        diversityPenalty = -0.50; // Penalizaci√≥n BRUTAL para > 4 usos
      } else if (usageCount > 3) {
        diversityPenalty = -0.35; // Penalizaci√≥n MUY fuerte para > 3 usos
      } else if (usageCount > 2) {
        diversityPenalty = -0.25; // Penalizaci√≥n fuerte para > 2 usos
      } else if (usageCount > 1) {
        diversityPenalty = -0.15; // Penalizaci√≥n media para > 1 uso
      }
      
      // Penalizaci√≥n por tasa de fallo alta
      const failurePenalty = failureRate > 2 ? -0.25 : failureRate > 1 ? -0.15 : 0;
      
      // Bonus ENORME por posiciones poco usadas - FASE 2
      const noveltyBonus = usageCount === 0 ? 0.30 : usageCount === 1 ? 0.15 : 0;
      
      // Bonus por posiciones con √©xito reciente
      const recentSuccessBonus = successRate > 0 && usageCount <= 2 ? 0.10 : 0;
      
      // NUEVO: Combinar Q-value con score adaptativo (40% peso adaptativo)
      const combinedQValue = (qValue * (1 - ADAPTIVE_WEIGHT)) + (adaptiveScore * ADAPTIVE_WEIGHT);
      
      const finalScore = combinedQValue + zoneBonus + diversityPenalty + noveltyBonus + failurePenalty + recentSuccessBonus;
      
      return {
        position: pos,
        qValue: combinedQValue,
        score: Math.max(0, Math.min(1, finalScore)), // Clamp entre 0-1
        usageCount,
        adaptiveScore, // Para debug
      };
    });

    // Ordenar por score descendente
    positionsWithScores.sort((a, b) => b.score - a.score);
    
    // Seleccionar entre top 12 para M√ÅXIMA variedad - FASE 2 (antes top 8)
    const topN = Math.min(12, positionsWithScores.length);
    const topCandidates = positionsWithScores.slice(0, topN);
    
    // Selecci√≥n ponderada: mayor probabilidad para mejores scores
    const totalScore = topCandidates.reduce((sum, c) => sum + c.score, 0);
    
    // Si todos tienen score muy bajo, forzar exploraci√≥n
    if (totalScore < 0.5) {
      console.log('‚ö†Ô∏è Scores muy bajos, forzando exploraci√≥n aleatoria');
      selectedPosition = finalAvailable[Math.floor(Math.random() * finalAvailable.length)];
    } else {
      let random = Math.random() * totalScore;
      
      let selected = topCandidates[0];
      for (const candidate of topCandidates) {
        random -= candidate.score;
        if (random <= 0) {
          selected = candidate;
          break;
        }
      }
      
      selectedPosition = selected.position;
    }
  }

  // Actualizar estado
  mlState.lastZoneUsed = targetZone;

  const qValue = mlState.positionQValues[selectedPosition] || 0.5;
  const confidence = Math.round(qValue * 100);

  const isHot = hotPositions.includes(selectedPosition);
  console.log(
    `ML: Pos ${selectedPosition} | ${strategy} | Zona ${targetZone} | Epsilon=${mlState.epsilon.toFixed(3)} | Q=${qValue.toFixed(3)}${isHot ? ' üî• CALIENTE' : ''}`
  );

  return {
    position: selectedPosition,
    strategy,
    zone: targetZone,
    epsilon: mlState.epsilon,
    qValue,
    confidence,
  };
}

/**
 * Actualizar Q-value despu√©s de una partida (aprendizaje)
 */
export async function updateMLFromGame(
  position: number,
  wasSuccess: boolean,
  reward: number
) {
  // Recompensa: +1 por victoria, -1 por derrota
  const immediateReward = wasSuccess ? reward : -reward;

  // Obtener Q-value actual
  const currentQ = mlState.positionQValues[position] || 0.5;

  // Obtener mejor Q-value de posiciones disponibles (para siguiente estado)
  const allQValues = Object.values(mlState.positionQValues);
  const maxNextQ = Math.max(...allQValues, 0.5);

  // F√≥rmula Q-Learning: Q(s,a) = Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]
  const newQ =
    currentQ +
    LEARNING_RATE * (immediateReward + DISCOUNT_FACTOR * maxNextQ - currentQ);

  // Actualizar Q-value
  mlState.positionQValues[position] = Math.max(0, Math.min(1, newQ)); // Clamp entre 0-1

  // Actualizar tasa de √©xito
  if (!mlState.positionSuccessRate[position]) {
    mlState.positionSuccessRate[position] = { wins: 0, total: 0 };
  }
  mlState.positionSuccessRate[position].total++;
  if (wasSuccess) {
    mlState.positionSuccessRate[position].wins++;
  }

  // Actualizar secuencia de posiciones seguras
  if (wasSuccess) {
    mlState.consecutiveSafePositions.unshift(position);
    if (mlState.consecutiveSafePositions.length > SAFE_SEQUENCE_LENGTH) {
      mlState.consecutiveSafePositions.pop();
    }
  }

  // Degradar epsilon (menos exploraci√≥n con el tiempo)
  mlState.epsilon = Math.max(MIN_EPSILON, mlState.epsilon * EPSILON_DECAY);

  // Incrementar contador de partidas
  mlState.totalGames++;

  console.log(
    `ML Actualizado: Pos ${position} | ${wasSuccess ? 'EXITO' : 'FALLO'} | Q: ${currentQ.toFixed(3)} -> ${mlState.positionQValues[position].toFixed(3)} | Epsilon: ${mlState.epsilon.toFixed(3)}`
  );
}

/**
 * Obtener estad√≠sticas del ML
 */
export function getMLStats() {
  const topPositions = Object.entries(mlState.positionQValues)
    .sort(([, a], [, b]) => b - a)
    .slice(0, 10)
    .map(([pos, qValue]) => ({
      position: parseInt(pos),
      qValue: qValue.toFixed(3),
      successRate: mlState.positionSuccessRate[parseInt(pos)]
        ? (
            (mlState.positionSuccessRate[parseInt(pos)].wins /
              mlState.positionSuccessRate[parseInt(pos)].total) *
            100
          ).toFixed(1) + '%'
        : 'N/A',
    }));

  return {
    totalGames: mlState.totalGames,
    epsilon: mlState.epsilon.toFixed(3),
    explorationCount: mlState.explorationCount,
    exploitationCount: mlState.totalGames - mlState.explorationCount,
    lastZoneUsed: mlState.lastZoneUsed,
    consecutiveSafePositions: mlState.consecutiveSafePositions,
    topPositions,
    learningRate: LEARNING_RATE,
    discountFactor: DISCOUNT_FACTOR,
    minEpsilon: MIN_EPSILON,
  };
}

/**
 * Resetear estado del ML (para testing)
 */
export function resetMLState() {
  mlState = {
    epsilon: 0.3,
    totalGames: 0,
    consecutiveSafePositions: [],
    lastZoneUsed: 'ZONE_A',
    positionQValues: {},
    positionSuccessRate: {},
    explorationCount: 0,
  };
  initializeMLState();
}
